{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ✅ STEP 1: Install Gradio\n",
        "!pip install -q gradio\n",
        "\n",
        "# ✅ STEP 2: Import Libraries\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (Conv2D, MaxPooling2D, BatchNormalization,\n",
        "                                     Dropout, Dense, GlobalAveragePooling2D)\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from google.colab import drive\n",
        "\n",
        "# ✅ STEP 3: Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ✅ STEP 4: Constants\n",
        "IMG_SIZE = 128\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 10\n",
        "DATASET_PATH = '/content/drive/MyDrive/brain_tumor/Training'\n",
        "CATEGORIES = ['glioma', 'meningioma', 'notumor', 'pituitary']\n",
        "\n",
        "# ✅ STEP 5: Load and preprocess data\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "for category in CATEGORIES:\n",
        "    folder_path = os.path.join(DATASET_PATH, category)\n",
        "    print(f\"Loading images for: {category}\")\n",
        "    for img_name in os.listdir(folder_path):\n",
        "        img_path = os.path.join(folder_path, img_name)\n",
        "        try:\n",
        "            image = cv2.imread(img_path)\n",
        "            if image is not None:\n",
        "                image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "                data.append(image)\n",
        "                labels.append(category)\n",
        "        except Exception as e:\n",
        "            print(f\"Skipped {img_path}: {e}\")\n",
        "\n",
        "# ✅ Convert to NumPy arrays and normalize\n",
        "data = np.array(data) / 255.0\n",
        "\n",
        "# ✅ Encode labels\n",
        "le = LabelEncoder()\n",
        "labels_encoded = le.fit_transform(labels)\n",
        "labels_categorical = to_categorical(labels_encoded, num_classes=len(CATEGORIES))\n",
        "\n",
        "# ✅ Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels_categorical, test_size=0.2, random_state=42)\n",
        "\n",
        "# ✅ STEP 6: Build CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(CATEGORIES), activation='softmax')\n",
        "])\n",
        "\n",
        "# ✅ Compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
        "\n",
        "# ✅ Train model\n",
        "history = model.fit(X_train, y_train, batch_size=BATCH_SIZE,\n",
        "                    validation_data=(X_test, y_test), epochs=EPOCHS, verbose=1)\n",
        "\n",
        "# ✅ STEP 7: Prediction function for Gradio\n",
        "def predict_image(image):\n",
        "    image = image.resize((IMG_SIZE, IMG_SIZE))\n",
        "    image = np.array(image)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "    image = image / 255.0\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    prediction = model.predict(image)\n",
        "    label = le.inverse_transform(np.argmax(prediction, axis=1))[0]\n",
        "    return {cat: float(prediction[0][i]) for i, cat in enumerate(le.classes_)}\n",
        "\n",
        "# ✅ STEP 8: Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=predict_image,\n",
        "    inputs=gr.Image(type='pil'),\n",
        "    outputs=gr.Label(num_top_classes=4),\n",
        "    title=\"Brain Tumor Classifier\",\n",
        "    description=\"Upload a brain MRI image to classify it as glioma, meningioma, notumor, or pituitary.\"\n",
        ")\n",
        "\n",
        "# ✅ STEP 9: Launch Gradio app\n",
        "interface.launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "u-wMa0csj5QE",
        "outputId": "16fa2b5c-33c4-4bd3-8694-545dddec85b6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Loading images for: glioma\n",
            "Loading images for: meningioma\n",
            "Loading images for: notumor\n",
            "Loading images for: pituitary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 678ms/step - accuracy: 0.6528 - loss: 0.8813 - val_accuracy: 0.2616 - val_loss: 6.5947\n",
            "Epoch 2/10\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 712ms/step - accuracy: 0.7591 - loss: 0.6267 - val_accuracy: 0.6028 - val_loss: 1.6707\n",
            "Epoch 3/10\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m256s\u001b[0m 692ms/step - accuracy: 0.7955 - loss: 0.5423 - val_accuracy: 0.6474 - val_loss: 0.9010\n",
            "Epoch 4/10\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 701ms/step - accuracy: 0.8345 - loss: 0.4509 - val_accuracy: 0.2598 - val_loss: 8.9148\n",
            "Epoch 5/10\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 693ms/step - accuracy: 0.8338 - loss: 0.4271 - val_accuracy: 0.4453 - val_loss: 2.0460\n",
            "Epoch 6/10\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 687ms/step - accuracy: 0.8396 - loss: 0.4207 - val_accuracy: 0.6702 - val_loss: 1.1396\n",
            "Epoch 7/10\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 664ms/step - accuracy: 0.8671 - loss: 0.3619 - val_accuracy: 0.8661 - val_loss: 0.3661\n",
            "Epoch 8/10\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 698ms/step - accuracy: 0.8754 - loss: 0.3525 - val_accuracy: 0.7305 - val_loss: 0.8774\n",
            "Epoch 9/10\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 682ms/step - accuracy: 0.8827 - loss: 0.3215 - val_accuracy: 0.8241 - val_loss: 0.4334\n",
            "Epoch 10/10\n",
            "\u001b[1m286/286\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 699ms/step - accuracy: 0.8914 - loss: 0.2948 - val_accuracy: 0.4479 - val_loss: 5.5899\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ad542a853106532dbe.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ad542a853106532dbe.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    }
  ]
}